# Implementation Improvements Based on README_relation_gating.md

## ‚úÖ Changes Implemented

### 1. **Time Normalization Range: [0, 100] ‚Üí [-1, 1]**

**File:** `preprocess_continuous_time.py`

**Before:**

```python
normalized_timestamps = (real_timestamps - t_min) / (t_max - t_min) * 100.0
```

**After:**

```python
normalized_timestamps = 2.0 * (real_timestamps - t_min) / (t_max - t_min) - 1.0
```

**Why:** README Section 2.2 specifies œÑ ‚àà [-1, 1] for proper cos() behavior and symmetry.

---

### 2. **Alpha Initialization: 0.5 ‚Üí 0.0**

**File:** `models.py`

**Before:**

```python
nn.init.constant_(self.alpha.weight, 0.5)  # Initialize to 0.5
```

**After:**

```python
nn.init.constant_(self.alpha.weight, 0.0)  # sigmoid(0) = 0.5
```

**Why:** README Section 3.1 - Initialize raw logits to 0, which gives sigmoid(0)=0.5 as baseline.

---

### 3. **Gating Formula: Explicit Ones Vector**

**File:** `models.py`

**Before:**

```python
gate = alpha * m + (1 - alpha)
```

**After:**

```python
gate = alpha * m + (1 - alpha) * torch.ones_like(m)
```

**Why:** README Section 2.3 - G(m, r) = Œ±_r _ m + (1 - Œ±_r) _ **1** (ones vector, not scalar).
This ensures proper broadcasting and mathematical correctness.

---

### 4. **Scoring Function: L1 Norm**

**File:** `models.py`

**Before:**

```python
score = -torch.abs(interaction).sum(dim=-1)
```

**After:**

```python
score = -torch.norm(interaction, p=1, dim=-1)
```

**Why:** README Section 2.4 uses |¬∑| notation for L1 norm. Using `torch.norm(p=1)` is cleaner.

---

### 5. **Alpha Polarization Regularizer (NEW)**

**File:** `regularizers.py`

**Added:**

```python
class AlphaPolarization(Regularizer):
    """
    Encourages alpha_r ‚Üí 0 (static) or 1 (dynamic).
    Uses binary entropy: H = -Œ±*log(Œ±) - (1-Œ±)*log(1-Œ±)
    """
```

**Why:** README Section 4 - "Regularization to push alpha_r toward 0 or 1"
Minimizes entropy to maximize polarization.

---

### 6. **Training Integration**

**Files:** `learner.py`, `optimizers.py`, `train_continuous_pairre.ps1`

**Added:**

- `--alpha_reg 0.01` argument and regularizer
- Alpha loss component in training loop
- Display in progress bar: `alpha=0.XXXX`

**Training command now includes:**

```powershell
--emb_reg 0.01          # N3 regularization
--time_reg 0.01         # Time embedding L3
--smoothness_reg 0.001  # W/b smoothness
--alpha_reg 0.01        # Alpha polarization (NEW)
```

---

## üöÄ How to Use

### Step 1: Re-preprocess Data

The time normalization range changed, so you MUST re-run preprocessing:

```powershell
cd external/tkbc/scripts
.\repreprocess_time.ps1
```

This will regenerate `ts_normalized.pickle` with [-1, 1] range.

### Step 2: Train Model

```powershell
.\train_continuous_pairre.ps1
```

### Step 3: Monitor Training

Watch for:

- **Alpha statistics**: Mean should move away from 0.5 toward extremes (0 or 1)
- **Polarization**: More static (<0.3) and dynamic (>0.7) relations over time
- **Loss components**:
  - `loss`: prediction loss
  - `reg`: embedding regularization
  - `cont`: time regularization
  - `smooth`: continuity smoothness
  - `alpha`: polarization loss ‚Üê NEW!

### Step 4: Validate Results

```powershell
.\check_continuity.ps1    # Check m(t) smoothness
.\visualize_alpha.ps1     # Analyze learned alphas
```

---

## üìä Expected Improvements

Based on README promises:

1. **Better MRR**: Static relations protected from noise
2. **Clearer Separation**: Œ± ‚âà 0 for static, Œ± ‚âà 1 for dynamic
3. **Smoother m(t)**: With [-1, 1] range and proper gating
4. **Improved Generalization**: Both interpolation and extrapolation

---

## üî¨ Technical Details

### Gating Behavior

| Alpha (Œ±_r) | Gating            | Behavior                 |
| ----------- | ----------------- | ------------------------ |
| 0.0         | **1** (constant)  | Static - ignores time    |
| 0.5         | 0.5m + 0.5        | Partial temporal         |
| 1.0         | **m** (full time) | Dynamic - fully temporal |

### Time Embedding Range

With œÑ ‚àà [-1, 1] and m = cos(W¬∑œÑ + b):

- Symmetric around 0
- Better gradient flow
- Matches README specification

### Polarization Mechanism

Binary entropy H(Œ±) is minimized:

- H(0) = 0 ‚Üê polarized (static)
- H(0.5) = 0.693 ‚Üê uncertain
- H(1) = 0 ‚Üê polarized (dynamic)

Regularizer pushes Œ± away from 0.5 toward 0 or 1.

---

## ‚ö†Ô∏è Important Notes

1. **Must re-run preprocessing** - Old [0, 100] data incompatible with [-1, 1] model
2. **Checkpoints invalid** - Previous models used wrong initialization and gating
3. **Monitor alpha_reg** - Too high (>0.1) may force premature polarization
4. **Patience** - Polarization takes several epochs to emerge

---

## üìù Files Modified

1. ‚úÖ `preprocess_continuous_time.py` - [-1, 1] normalization
2. ‚úÖ `models.py` - Alpha init, gating formula, scoring
3. ‚úÖ `regularizers.py` - AlphaPolarization class
4. ‚úÖ `learner.py` - Alpha regularizer integration
5. ‚úÖ `optimizers.py` - Alpha loss in training loop
6. ‚úÖ `train_continuous_pairre.ps1` - Updated hyperparameters

## üéØ Ready to Train!

All README improvements implemented. Model now matches theoretical specification exactly.
