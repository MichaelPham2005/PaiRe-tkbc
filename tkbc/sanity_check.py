#!/usr/bin/env python3
# Sanity Check Script for ContinuousPairRE before training
# Ki·ªÉm tra c√°c ƒëi·ªÉm quan tr·ªçng tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu train

import os
import torch
from pathlib import Path
from datasets import TemporalDataset
from models import ContinuousPairRE

def sanity_check():
    print("="*70)
    print("SANITY CHECK - ContinuousPairRE")
    print("="*70)
    
    # 1. Check W and b dimensions in ContinuousTimeEmbedding
    print("\n1. Ki·ªÉm tra k√≠ch th∆∞·ªõc W v√† b trong ContinuousTimeEmbedding:")
    rank = 100
    dataset = TemporalDataset('ICEWS14', use_continuous_time=True)
    sizes = dataset.get_shape()
    model = ContinuousPairRE(sizes, rank).cuda()
    
    W_shape = model.time_encoder.W.shape
    b_shape = model.time_encoder.b.shape
    print(f"   ‚úì W shape: {W_shape} (Expected: torch.Size([{rank}]))")
    print(f"   ‚úì b shape: {b_shape} (Expected: torch.Size([{rank}]))")
    
    if W_shape == torch.Size([rank]) and b_shape == torch.Size([rank]):
        print("   ‚úì PASS: W v√† b ƒë·ªÅu l√† vector c√≥ chi·ªÅu b·∫±ng rank")
        print("   ‚Üí M√¥ h√¨nh c√≥ th·ªÉ h·ªçc c√°c chu k·ª≥ kh√°c nhau tr√™n t·ª´ng chi·ªÅu")
    else:
        print("   ‚úó FAIL: W ho·∫∑c b kh√¥ng c√≥ k√≠ch th∆∞·ªõc ƒë√∫ng!")
        return False
    
    # 2. Check data paths and files
    print("\n2. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu v√† files:")
    data_path = Path(__file__).resolve().parent / "data" / "ICEWS14"
    
    required_files = [
        'train.pickle',
        'valid.pickle', 
        'test.pickle',
        'ts_normalized.pickle',
        'to_skip.pickle'
    ]
    
    all_exist = True
    for filename in required_files:
        file_path = data_path / filename
        exists = file_path.exists()
        status = "‚úì" if exists else "‚úó"
        print(f"   {status} {filename}: {'Found' if exists else 'NOT FOUND'}")
        if not exists:
            all_exist = False
    
    if all_exist:
        print("   ‚úì PASS: T·∫•t c·∫£ files c·∫ßn thi·∫øt ƒë·ªÅu t·ªìn t·∫°i")
    else:
        print("   ‚úó FAIL: Thi·∫øu m·ªôt s·ªë files quan tr·ªçng!")
        print("   ‚Üí H√£y ch·∫°y preprocess_continuous_time.py tr∆∞·ªõc")
        return False
    
    # 3. Check alpha initialization
    print("\n3. Ki·ªÉm tra kh·ªüi t·∫°o tham s·ªë alpha:")
    with torch.no_grad():
        alphas = torch.sigmoid(model.alpha.weight).cpu()
    
    print(f"   ‚úì Number of relations: {len(alphas)}")
    print(f"   ‚úì Alpha initialization:")
    print(f"      Mean: {alphas.mean().item():.4f}")
    print(f"      Std:  {alphas.std().item():.4f}")
    print(f"      Min:  {alphas.min().item():.4f}")
    print(f"      Max:  {alphas.max().item():.4f}")
    
    # Check if alphas are reasonable (not all 0 or all 1)
    if 0.3 < alphas.mean().item() < 0.7:
        print("   ‚úì PASS: Alpha ƒë∆∞·ª£c kh·ªüi t·∫°o ·ªü gi√° tr·ªã trung b√¨nh h·ª£p l√Ω (0.5)")
    else:
        print("   ‚ö† WARNING: Alpha mean kh√¥ng ·ªü kho·∫£ng 0.3-0.7")
    
    # 4. Test forward pass with continuous time
    print("\n4. Test forward pass v·ªõi continuous time:")
    try:
        # Create a small batch
        import numpy as np
        train_data = dataset.get_train()
        batch = train_data[:5]
        
        # Convert to continuous time
        batch_continuous = batch.copy().astype(np.float32)
        for i in range(5):
            ts_id = int(batch[i, 3])
            batch_continuous[i, 3] = dataset.ts_normalized[ts_id]
        
        batch_tensor = torch.from_numpy(batch_continuous).cuda()
        
        with torch.no_grad():
            scores, factors, time_embeds = model.forward(batch_tensor)
        
        print(f"   ‚úì Forward pass successful")
        print(f"   ‚úì Scores shape: {scores.shape}")
        print(f"   ‚úì Factors shapes: {[f.shape for f in factors]}")
        print(f"   ‚úì Time embeddings shape: {time_embeds.shape}")
        print("   ‚úì PASS: Forward pass ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng")
    except Exception as e:
        print(f"   ‚úó FAIL: Forward pass g·∫∑p l·ªói: {e}")
        return False
    
    # 5. Check optimizer configuration
    print("\n5. Ki·ªÉm tra optimizer configuration:")
    from torch import optim
    from regularizers import N3, ContinuousTimeLambda3
    
    opt = optim.Adagrad(model.parameters(), lr=0.1)
    emb_reg = N3(0.001)
    time_reg = ContinuousTimeLambda3(0.001)
    
    print(f"   ‚úì Optimizer: Adagrad (learning rate: 0.1)")
    print(f"   ‚úì Embedding regularizer: N3 (weight: 0.001)")
    print(f"   ‚úì Time regularizer: ContinuousTimeLambda3 (weight: 0.001)")
    print("   ‚úì PASS: Optimizer v√† regularizers ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng")
    
    # 6. Check model parameters
    print("\n6. Ki·ªÉm tra model parameters:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"   ‚úì Total parameters: {total_params:,}")
    print(f"   ‚úì Trainable parameters: {trainable_params:,}")
    
    # Break down by component
    entity_params = model.entity_embeddings.weight.numel()
    rel_head_params = model.relation_head.weight.numel()
    rel_tail_params = model.relation_tail.weight.numel()
    time_W_params = model.time_encoder.W.numel()
    time_b_params = model.time_encoder.b.numel()
    alpha_params = model.alpha.weight.numel()
    
    print(f"\n   Parameter breakdown:")
    print(f"      Entity embeddings: {entity_params:,}")
    print(f"      Relation head: {rel_head_params:,}")
    print(f"      Relation tail: {rel_tail_params:,}")
    print(f"      Time encoder W: {time_W_params:,}")
    print(f"      Time encoder b: {time_b_params:,}")
    print(f"      Alpha (gating): {alpha_params:,}")
    
    # 7. Final recommendations
    print("\n" + "="*70)
    print("KHUY·∫æN NGH·ªä TR∆Ø·ªöC KHI TRAIN:")
    print("="*70)
    print("\n‚úì T·∫•t c·∫£ ki·ªÉm tra ƒë√£ PASS!")
    print("\nüìù L∆∞u √Ω khi training:")
    print("   1. Theo d√µi alpha statistics m·ªói epoch")
    print("   2. N·∫øu alpha h·ªôi t·ª• qu√° nhanh v·ªÅ 0 ho·∫∑c 1:")
    print("      ‚Üí Gi·∫£m learning rate ho·∫∑c th√™m regularization cho alpha")
    print("   3. N·∫øu overfitting (train MRR >> valid MRR):")
    print("      ‚Üí TƒÉng --emb_reg v√† --time_reg (th·ª≠ 0.01, 0.1)")
    print("   4. N·∫øu underfitting (train v√† valid MRR ƒë·ªÅu th·∫•p):")
    print("      ‚Üí TƒÉng rank ho·∫∑c gi·∫£m regularization")
    print("   5. Monitor loss components:")
    print("      ‚Üí loss: prediction loss")
    print("      ‚Üí reg: embedding regularization")
    print("      ‚Üí cont: time regularization")
    print("\nüöÄ S·∫µn s√†ng train! Ch·∫°y l·ªánh:")
    print("   .\\train_continuous_pairre.ps1")
    print("="*70)
    
    return True

if __name__ == "__main__":
    try:
        success = sanity_check()
        exit(0 if success else 1)
    except Exception as e:
        print(f"\n‚úó CRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
